<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<!-- original template from from url=(0035)http://www.cs.berkeley.edu/~barron/ -->
<html><head><meta http-equiv="Content-Type" content="text/html; charset=windows-1252">
	<meta name="viewport" content="width=800">
    <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
    <style type="text/css">
      /* Color scheme stolen from Sergey Karayev */
      a {
      color: #1772d0;
      text-decoration:none;
      }
      a:focus, a:hover {
      color: #f09228;
      text-decoration:none;
      }
      body,td,th {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 14px
      }
      strong {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 14px;
      }
      heading {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 18px;
      }
      papertitle {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 15px;
      font-weight: 700
      }
      name {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 32px;
      }
	.fade {
	   transition: opacity .2s ease-in-out;
	   -moz-transition: opacity .2s ease-in-out;
	   -webkit-transition: opacity .2s ease-in-out;
	   }
    </style>
	
    <link href='http://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
    <!-- <link rel="icon" type="image/png" href="http://www.cs.berkeley.edu/~barron/seal_icon.png"> -->
	
    <title>Xiaopeng Yan</title>
    
    <link href="/img/css" rel="stylesheet" type="text/css">
  </head>
  <body>
    <table width="800" border="0" align="center" cellspacing="0" cellpadding="0">
      <tbody><tr>
        <td>
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody><tr>
              <td width="68%" valign="middle">
                <p align="center">
                  <name>Xiaopeng yanxp</name>
                  
                </p><p align="">&nbsp;&nbsp;I am a master student in <a href="http://hcp.sysu.edu.cn">Human Cyber Physical Intelligence Integration Lab</a> at the  <a href="http://www.sysu.edu.cn" > SUN YAT-SEN UNIVERSITY</a>, led by Professor <a href="http://www.linliang.net/">Liang Lin</a>. I received a B.E. in automation at <a href="http://seit.sysu.edu.cn/">School of Electronics and Information Technology</a>. During my BE, I spent two months as an intern in the Big Data department at the China Telecom.
</br>
</br>
<!-- 
I am so delighted to start computer vision research with Professor <a href="https://explorecourses.stanford.edu/instructor/mehrdads">Mehrdad Shahshahani</a> at MI&V Lab (formerly  <a href="http://www.ipm.ac.ir/">IPM Vision Group</a>) at the <a href="http://www.sharif.edu">Sharif University</a>. I did my masters in AI at <a href="http://aut.ac.ir/aut/">Tehran Polytechnic</a> and my bachelors in Software Engineering at Shomal University in Amol (my <a href="https://en.wikipedia.org/wiki/Amol">hometown</a>). -->

<!--I had the opportunity to work under
I started Computer Vision with Mehrdad Shahshahani
Prior to my Ph.D., I spent one year as a research assistant at MI&V lab in <a href="http://www.test.com">Sharif University of Technology</a>. I was also fortunate enough to be advised by Professor <a href="http://www.test.com">Mehrdad Shahshahani</a> at the <a href="http://www.test.com">IPM Vision Group</a> from 2009 until 2011. I used to collaborate with IPPR lab at <a href="http://www.test.com">Amirkabir University of Technology</a> under supervision of Professor <a href="http://www.test.com">Mohammad Rahmati</a> as well.
I received my Master Degree on Artificial Intelligence from Tehran Polytechnic and my Bachelor Degree on Software Engineering from Shomal University at Amol (my home town). -->
                </p><p align="center">
<a href="mailto:moin.nabi@unitn.it">Email</a> &nbsp;/&nbsp;
<a href="./files/cv.pdf">CV</a> &nbsp;/&nbsp;
<!--<a href="https://scholar.google.it/citations?user=31seHAMAAAAJ&hl=en">Google Scholar</a> &nbsp;/&nbsp; -->
<!-- <a href="./files/Thesis_compressed.pdf">Thesis</a> &nbsp;/&nbsp;
<a href="https://scholar.google.com/citations?hl=en&user=31seHAMAAAAJ&view_op=list_works&sortby=pubdate">Google Scholar</a> &nbsp;/&nbsp; 
<a href="https://it.linkedin.com/pub/moin-nabi/3b/492/aa5"> LinkedIn </a> -->
                </p>
              </td>
              <!--<td width="33%"><img src="./img/moin_pic_cool.jpg"></td>-->
				<td> <img src="./img/moin_pic_cool_2.jpg" style="width: 200;"></td></tr>
          </tbody></table>
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody><tr>
              <td>
                <heading>Research</heading>
             <!--    <p> I work primarily on computer vision, but I am also interested in machine learning and pattern recognition. The central goal of my research is to use vast amounts of data to understand the underlying semantics and structure of visual contents. I am especially interested in learning and recognizing visual object categories and understanding human behaviors. I spent my Ph.D. working on learning mid-level representations for visual recognition (image and video understanding) and now, I am more focused on learning deep neural networks from noisy and incomplete multi-modal data.</p> -->
              </td>
            </tr>
          </tbody></table>
<!--SECTION -->

          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody><tr>
              <td>
                <heading>News</heading>
  <!--               <p> <strong>[2016.05.15]</strong> I will join <strong><a href="https://icn.sap.com/home.html">SAP Machine Learning Research</a></strong> in Berlin, as a <strong>Senior Research Scientist</strong> in Deep Learning.</p>
		<p> <strong>[2016.04.01]</strong> Two papers are accepted in <strong><a href="http://acl2017.org/">ACL 2017</a>.</strong> Congrats Azad and Ravi!</p>
                <p> <strong>[2016.12.01]</strong> I will present <strong><a href="https://arxiv.org/pdf/1611.06764.pdf">Plug-and-Play Binary Quantization Layer</a></strong> at Workshop on Efficient Deep Learning at NIPS 2016!</p>
		<p> <strong>[2016.11.11]</strong> The <strong><a href="https://github.com/hosseinm/med">Motion Emotion Dataset (MED)</a></strong> is online!</p>
                <p> <strong>[2016.09.25]</strong> Our work is finalist for the <strong><a href="http://2016.ieeeicip.org/Awards.asp">Best Paper Award</a></strong> in ICIP 2016.</p>

           -->    </td>
            </tr>
          </tbody></table>
<!--SECTION -->

          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody><tr>
              <td>
                <heading>Publications and Preprints</heading>
              </td>
            </tr>
          </tbody></table>


          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
		<tbody>
		   <tr>


<!--Next paper should be listed in bottom -->
	              <td width="25%"><img src="./img/SelfPaced.jpg" alt="PontTuset" width="200" style="border-style: none">
	              </td><td width="75%" valign="top">
	                <p><a href="https://arxiv.org/abs/1605.07651">
	<papertitle>Self-Paced Deep Learning for Weakly Supervised Object Detection</papertitle></a><br>E. Sangineto*, <strong>M. Nabi</strong>*, D. Culibrk and N. Sebe<br>
                  <em>arXiv:1605.07651v2</em>, 2017 &nbsp; <br>
                  <a href="https://arxiv.org/pdf/1605.07651.pdf">PDF</a> /
		  <a href="https://github.com/moinnabi/SelfPacedDeepLearning">code</a> /
                  <a href="http://dblp.uni-trier.de/rec/bib2/journals/corr/SanginetoNCS16.bib">bibtex</a>
                </p><p></p>
                <p>In this paper we propose a self-paced learning protocol for weakly-supervised object detection. The main idea is to iteratively select a subset of samples that are most likely correct, which are used for training. We show results on Pascal VOC and ImageNet, outperforming the previous state of the art on both datasets.
</br></br>
<small>*Authors contributed equally</small>

		</p><p></p>
                <p></p>
              </td>
            </tr>


<!--SECTION -->

          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody><tr>
              <td>
                <heading>Miscellaneous</heading>
              </td>
            </tr>
          </tbody></table>
          <table width="100%" align="center" border="0" cellpadding="20">
            <tbody><tr>


<!--Next paper should be listed in bottom -->
	              <td width="25%"><img src="./img/PhDthesis.jpg" alt="PontTuset" width="160" style="border-style: none">
	              </td><td width="75%" valign="top">
	                <p><a href="http://arxiv.org/abs/1512.07314">
	<papertitle>Mid-level Representation for Visual Recognition</papertitle></a><br>
                  <strong>Moin Nabi</strong><br>
                  <em>Ph.D. Dissertation </em>, 2015 &nbsp; <br>
                  <a href="./files/PhD_thesis.pdf">PDF</a> /
                  <a href="./files/thesis_slide.pdf">slides</a> /
                  <a href="https://www.youtube.com/watch?v=6IY-0swKaiM">talk</a> /
                </p><p></p>
                <p>This thesis targets employing mid-level representations for different high-level visual
recognition tasks, both in image and video understanding.
                </p><p></p>
                <p></p>
              </td>
            </tr>

          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody><tr>
              <td>
                <br>
                <p align="center"><font size="3">
			             copyright@2017 
                </p>
              </td>
            </tr>
          </tbody></table>
          <script type="text/javascript">
            var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
                    document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
                    
          </script><script src="./img/ga.js" type="text/javascript"></script> <script type="text/javascript">
            try {
                    var pageTracker = _gat._getTracker("UA-7580334-1");
                    pageTracker._trackPageview();
                    } catch(err) {}
                    
          </script>
        </td>
      </tr>
    </tbody></table>
  

</body></html>
