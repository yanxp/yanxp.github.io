<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<!-- original template from from url=(0035)http://www.cs.berkeley.edu/~barron/ -->
<html><head><meta http-equiv="Content-Type" content="text/html; charset=windows-1252">
	<meta name="viewport" content="width=800">
    <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
    <style type="text/css">
      /* Color scheme stolen from Sergey Karayev */
      a {
      color: #1772d0;
      text-decoration:none;
      }
      a:focus, a:hover {
      color: #f09228;
      text-decoration:none;
      }
      body,td,th {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 14px
      }
      strong {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 14px;
      }
      heading {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 18px;
      }
      papertitle {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 15px;
      font-weight: 700
      }
      name {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 32px;
      }
	.fade {
	   transition: opacity .2s ease-in-out;
	   -moz-transition: opacity .2s ease-in-out;
	   -webkit-transition: opacity .2s ease-in-out;
	   }
    </style>
	
    <link href='http://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
    <!-- <link rel="icon" type="image/png" href="http://www.cs.berkeley.edu/~barron/seal_icon.png"> -->
	
    <title>Xiaopeng Yan</title>
    
    <link href="/img/css" rel="stylesheet" type="text/css">
  </head>
  <body>
    <table width="800" border="0" align="center" cellspacing="0" cellpadding="0">
      <tbody><tr>
        <td>
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody><tr>
              <td width="68%" valign="middle">
                <p align="center">
                  <name>Xiaopeng yanxp</name>
                  
   <!--              </p><p align="">I am a postdoctoral research fellow in <a href="http://disi.unitn.it/~mhug/index.html">Deep Relational Learning</a> group at the University of Trento, led by Professor <a href="http://disi.unitn.it/~sebe/">Nicu Sebe</a>. I received a Ph.D. in computer science at <a href="http://www.iit.it/">Italian Institute of Technology(IIT)</a> where I was advised by Professor <a href="http://profs.sci.univr.it/~swan/">Vittorio Murino</a>. During my PhD, I spent a period as a visiting scholar in the <a href="https://www.cs.washington.edu/">CS department</a> at the University of Washington working with <a href="http://homes.cs.washington.edu/~ali/">Ali Farhadi</a>. I also had the privilege of working closely with Professor <a href="http://www0.cs.ucl.ac.uk/staff/m.pontil/">Massimiliano Pontil</a> from UCL. -->
</br>
</br>
<!-- 
I am so delighted to start computer vision research with Professor <a href="https://explorecourses.stanford.edu/instructor/mehrdads">Mehrdad Shahshahani</a> at MI&V Lab (formerly  <a href="http://www.ipm.ac.ir/">IPM Vision Group</a>) at the <a href="http://www.sharif.edu">Sharif University</a>. I did my masters in AI at <a href="http://aut.ac.ir/aut/">Tehran Polytechnic</a> and my bachelors in Software Engineering at Shomal University in Amol (my <a href="https://en.wikipedia.org/wiki/Amol">hometown</a>). -->

<!--I had the opportunity to work under
I started Computer Vision with Mehrdad Shahshahani
Prior to my Ph.D., I spent one year as a research assistant at MI&V lab in <a href="http://www.test.com">Sharif University of Technology</a>. I was also fortunate enough to be advised by Professor <a href="http://www.test.com">Mehrdad Shahshahani</a> at the <a href="http://www.test.com">IPM Vision Group</a> from 2009 until 2011. I used to collaborate with IPPR lab at <a href="http://www.test.com">Amirkabir University of Technology</a> under supervision of Professor <a href="http://www.test.com">Mohammad Rahmati</a> as well.
I received my Master Degree on Artificial Intelligence from Tehran Polytechnic and my Bachelor Degree on Software Engineering from Shomal University at Amol (my home town). -->
                </p><p align="center">
<a href="mailto:moin.nabi@unitn.it">Email</a> &nbsp;/&nbsp;
<a href="./files/cv.pdf">CV</a> &nbsp;/&nbsp;
<!--<a href="https://scholar.google.it/citations?user=31seHAMAAAAJ&hl=en">Google Scholar</a> &nbsp;/&nbsp; -->
<!-- <a href="./files/Thesis_compressed.pdf">Thesis</a> &nbsp;/&nbsp;
<a href="https://scholar.google.com/citations?hl=en&user=31seHAMAAAAJ&view_op=list_works&sortby=pubdate">Google Scholar</a> &nbsp;/&nbsp; 
<a href="https://it.linkedin.com/pub/moin-nabi/3b/492/aa5"> LinkedIn </a> -->
                </p>
              </td>
              <!--<td width="33%"><img src="./img/moin_pic_cool.jpg"></td>-->
				<td> <img src="./img/moin_pic_cool_2.jpg" style="width: 200;"></td></tr>
          </tbody></table>
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody><tr>
              <td>
                <heading>Research</heading>
             <!--    <p> I work primarily on computer vision, but I am also interested in machine learning and pattern recognition. The central goal of my research is to use vast amounts of data to understand the underlying semantics and structure of visual contents. I am especially interested in learning and recognizing visual object categories and understanding human behaviors. I spent my Ph.D. working on learning mid-level representations for visual recognition (image and video understanding) and now, I am more focused on learning deep neural networks from noisy and incomplete multi-modal data.</p> -->
              </td>
            </tr>
          </tbody></table>
<!--SECTION -->

          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody><tr>
              <td>
                <heading>News</heading>
  <!--               <p> <strong>[2016.05.15]</strong> I will join <strong><a href="https://icn.sap.com/home.html">SAP Machine Learning Research</a></strong> in Berlin, as a <strong>Senior Research Scientist</strong> in Deep Learning.</p>
		<p> <strong>[2016.04.01]</strong> Two papers are accepted in <strong><a href="http://acl2017.org/">ACL 2017</a>.</strong> Congrats Azad and Ravi!</p>
                <p> <strong>[2016.12.01]</strong> I will present <strong><a href="https://arxiv.org/pdf/1611.06764.pdf">Plug-and-Play Binary Quantization Layer</a></strong> at Workshop on Efficient Deep Learning at NIPS 2016!</p>
		<p> <strong>[2016.11.11]</strong> The <strong><a href="https://github.com/hosseinm/med">Motion Emotion Dataset (MED)</a></strong> is online!</p>
                <p> <strong>[2016.09.25]</strong> Our work is finalist for the <strong><a href="http://2016.ieeeicip.org/Awards.asp">Best Paper Award</a></strong> in ICIP 2016.</p>

           -->    </td>
            </tr>
          </tbody></table>
<!--SECTION -->

          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody><tr>
              <td>
                <heading>Publications and Preprints</heading>
              </td>
            </tr>
          </tbody></table>


          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
		<tbody>
		   <tr>


<!--Next paper should be listed in bottom -->
	              <td width="25%"><img src="./img/SelfPaced.jpg" alt="PontTuset" width="200" style="border-style: none">
	              </td><td width="75%" valign="top">
	                <p><a href="https://arxiv.org/abs/1605.07651">
	<papertitle>Self-Paced Deep Learning for Weakly Supervised Object Detection</papertitle></a><br>E. Sangineto*, <strong>M. Nabi</strong>*, D. Culibrk and N. Sebe<br>
                  <em>arXiv:1605.07651v2</em>, 2017 &nbsp; <br>
                  <a href="https://arxiv.org/pdf/1605.07651.pdf">PDF</a> /
		  <a href="https://github.com/moinnabi/SelfPacedDeepLearning">code</a> /
                  <a href="http://dblp.uni-trier.de/rec/bib2/journals/corr/SanginetoNCS16.bib">bibtex</a>
                </p><p></p>
                <p>In this paper we propose a self-paced learning protocol for weakly-supervised object detection. The main idea is to iteratively select a subset of samples that are most likely correct, which are used for training. We show results on Pascal VOC and ImageNet, outperforming the previous state of the art on both datasets.
</br></br>
<small>*Authors contributed equally</small>

		</p><p></p>
                <p></p>
              </td>
            </tr>

<!--Next paper should be listed in bottom -->
	              <td width="25%"><img src="./img/CNNPlugPlay.jpg" alt="PontTuset" width="200" style="border-style: none">
	              </td><td width="75%" valign="top">
	                <p><a href="https://arxiv.org/abs/1610.00307">
	<papertitle>Plug-and-Play CNN for Crowd  Analysis: An Application in Abnormal Event Detection</papertitle></a><br>
			M. Ravanbakhsh, <strong>M. Nabi</strong>, Mousavi, E. Sangineto and N. Sebe<br>
                  <em>arXiv:1610.00307</em>, 2016 &nbsp; <br>
                  <a href="https://arxiv.org/pdf/1610.00307v1.pdf">PDF</a> /
                  <a href="#">bibtex</a>
                </p><p></p>
                <p>We present a novel measure-based method which allows measuring the local abnormality in a video by combining semantic information (inherited from existing CNN models) with low-level Optical Flow. The proposed method is validated on challenging abnormality detection datasets and the showed the superiority of our method compared to the state-of-the-arts.
                </p><p></p>
                <p></p>
              </td>
            </tr>


<!--Next paper should be listed in bottom -->
	              <td width="25%"><img src="./img/Emotion.jpg" alt="PontTuset" width="200" style="border-style: none">
	              </td><td width="75%" valign="top">
	                <p><a href="https://arxiv.org/abs/1607.07646">
	<papertitle>Emotion-Based Crowd Representation for Abnormality Detection</papertitle></a><br>
			H.R. Rabiee, J. Haddadnia, H. Mousavi, <strong>M. Nabi</strong>, V. Murino and N. Sebe<br>
                  <em>arXiv:1607.07646</em>, 2016 &nbsp; <br>
                  <a href="https://arxiv.org/pdf/1607.07646v1">PDF</a> /
		  <a href="https://github.com/hosseinm/med">dataset</a> /
                  <a href="http://dblp.uni-trier.de/rec/bib2/journals/corr/RabieeHMNMS16.bib">bibtex</a>
                </p><p></p>
                <p>In this paper, we proposed a novel crowd dataset with both annotations of abnormal crowd behavior and crowd emotion. We also presented a method which exploits jointly the complimentary information of these two task, outperforming all baselines of both tasks significantly.
                </p><p></p>
                <p></p>
              </td>
            </tr>

<!--Next paper should be listed in bottom -->
	              <td width="25%"><img src="./img/BrainDecode.jpg" alt="PontTuset" width="200" style="border-style: none">
	              </td><td width="75%" valign="top">
	                <p><a href="#">
	<papertitle>Sparse-coded Cross-domain Adaptation from the Visual to the Brain Domain</papertitle></a><br>
			P. Ghaemmaghami, <strong>M. Nabi</strong>, Y. Yan and N. Sebe<br>
                  <em>IEEE International Conference on Pattern Recognition (ICPR)</em>, 2016 &nbsp; <font color="red"><strong>(Oral)</strong></font><br>
                  <a href="./files/ICPR2016_paper.pdf">PDF</a> /
                  <a href="#">slies</a> /
                  <a href="./files/ICPR2016.bib">bibtex</a>
                </p><p></p>
                <p>In this paper, an adaptation paradigm is employed in order to transfer knowledge from visual domain to brain domain. We experimentally show that such adaptation procedure leads to improved results for the object recognition task in the brain domain, outperforming significantly the results achieved by the brain features alone.
                </p><p></p>
                <p></p>
              </td>
            </tr>

<!--Next paper should be listed in bottom -->
	              <td width="25%"><img src="./img/FineGrained.jpg" alt="PontTuset" width="200" style="border-style: none">
	              </td><td width="75%" valign="top">
	                <p><a href="http://ieeexplore.ieee.org/document/7738074/">
	<papertitle>Novel Dataset for Fine-grained Abnormal Behavior Understanding in Crowd</papertitle></a><br>
			H.R. Rabiee, J. Haddadnia, H. Mousavi, M. Kalantarzadeh, <strong>M. Nabi</strong>, V. Murino<br>
                  <em> IEEE Advanced Video and Signal-based Surveillance (AVSS)</em>, 2016 &nbsp; <br>
                  <a href="./files/AVSS2016_paper.pdf">PDF</a> /
                  <a href="./files/AVSS2016_poster.pdf">poster</a> /
                  <a href="https://github.com/hosseinm/med">dataset</a> /
                  <a href="./files/AVSS2016.bib">bibtex</a>
                </p><p></p>
                <p>This work presents a novel crowd dataset annotated by fine-grained abnormal behavior categoriy labels. We also evaluated two state-of-the-art methods on our dataset, showing that our dataset can be effectively used as a benchmark for the fine-grained abnormality detection problem.
                </p><p></p>
                <p></p>
              </td>
            </tr>


<!--Next paper should be listed in bottom -->
	              <td width="25%"><img src="./img/CNNaware.jpg" alt="PontTuset" width="200" style="border-style: none">
	              </td><td width="75%" valign="top">
	                <p><a href="https://arxiv.org/abs/1609.09220">
	<papertitle>CNN-aware Binary Map For General Image Segmentation</papertitle></a><br>
	M. Ravanbakhsh, H. Mousavi, <strong>M. Nabi</strong>, M. Rastegari and C. Regazzoni<br>
                  <em>IEEE International Conference on Image Processing (ICIP)</em>, 2016 &nbsp; <br>
		<font color="red"><strong>Best Paper / Student Paper Award Finalist (7 / ~2000 submissions)</strong></font><br>
                  <a href="./files/ICIP2016_paper.pdf">PDF</a> /
                  <a href="./files/ICIP2016_poster.pdf">poster</a> /
                  <a href="./files/ICIP2016_slide.pdf">slides</a> /
                  <a href="https://github.com/matt-rb">code</a> /
                  <a href="./files/ICIP2016.bib">bibtex</a>
                </p><p></p>
                <p>In this paper we introduce a novel method for general semantic segmentation that can benefit from general semantics of Convolutional Neural Network (CNN). Experiments show that our segmentation algorithm outperform the state-of-the-art non-semantic segmentation methods by large margin.
                </p><p></p>
                <p></p>
              </td>
            </tr>

<!--Next paper should be listed in bottom -->
	              <td width="25%"><img src="./img/SubModels.PNG" alt="PontTuset" width="200" style="border-style: none">
	              </td><td width="75%" valign="top">
	                <p><a href="http://ieeexplore.ieee.org/document/7298988/?reload=true&arnumber=7298988">
	<papertitle>Learning with Dataset Bias in Latent Subcategory Models</papertitle></a><br>
                  D. Stamos, S. Martelli, <strong>M. Nabi</strong>, A. McDonald, V. Murino, M. Pontil<br>
                  <em>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2015 &nbsp; <br>
                  <a href="./files/CVPR2015_paper.pdf">PDF</a> /
                  <a href="./files/CVPR2015_abstract.pdf">abstract</a> /
                  <a href="./files/CVPR2015.bib">bibtex</a>
                </p><p></p>
                <p>We present a multi-task learning framework which provides a means to borrow statistical strength from the datasets while reducing their inherent bias. In experiments we demonstrate that our method, when tested on PASCAL, LabelMe, Caltech101 and SUN in a leave-one-dataset-out fashion, achieves an average improvement of over 6.5% over state-of-the-arts.
                </p><p></p>
                <p></p>
              </td>
            </tr>

<!--Next paper should be listed in bottom -->
	              <td width="25%"><img src="./img/CM.jpg" alt="PontTuset" width="200" style="border-style: none">
	              </td><td width="75%" valign="top">
	                <p><a href="http://ieeexplore.ieee.org/document/7351223/">
	<papertitle>Crowd Motion Monitoring Using Tracklet-based Commotion Measure</papertitle></a><br>
<strong>M. Nabi</strong>*, H. Mousavi*, H. Kiani, A. Perina and V. Murino<br>
                  <em>IEEE International Conference on Image Processing (ICIP)</em>, 2015 &nbsp; <br>
                  <a href="./files/ICIP2015_paper.pdf">PDF</a> /
                  <a href="./files/ICIP2015_poster.pdf">poster</a> /
                  <a href="./files/ICIP2015_video.mp4">video</a> /
                  <a href="./files/ICIP2015.bib">bibtex</a>
                </p><p></p>
                <p>We present a tracklet-based measure to capture the commotion of a crowd motion for the task of abnormality detection in crowd.
</br></br>
<small>*Authors contributed equally</small>

		</p><p></p>
                <p></p>
              </td>
            </tr>

<!--Next paper should be listed in bottom -->
	              <td width="25%"><img src="./img/iHOT.jpg" alt="PontTuset" width="200" style="border-style: none">
	              </td><td width="75%" valign="top">
	                <p><a href="http://link.springer.com/chapter/10.1007%2F978-3-319-23234-8_66#page-1">
	<papertitle>Abnormality Detection with Improved Histogram of Oriented Tracklets</papertitle></a><br>
                  H. Mousavi, <strong>M. Nabi</strong>, H. Kiani, A. Perina and V. Murino<br>
                  <em>International Conference on Image Analysis and Processing (ICIAP)</em>, 2015 &nbsp; <br>
                  <a href="./files/ICIAP2015_paper.pdf">PDF</a> /
                  <a href="./files/ICIAP2015_poster.pdf">poster</a> /
                  <a href="./files/ICIAP2015.bib">bibtex</a>
                </p><p></p>
                <p>In this paper, we presented an efficient video descriptor for the task of abnormality detection in the crowded environments and achieved the-state-of-the-art results on the available datasets.
                </p><p></p>
                <p></p>
              </td>
            </tr>

<!--Next paper should be listed in bottom -->
	              <td width="25%"><img src="./img/WeblyPatch_2.jpg" alt="PontTuset" width="200" style="border-style: none">
	              </td><td width="75%" valign="top">
	                <p><a href="#">
	<papertitle>Webly-supervised Subcategoty-aware Discriminative Patch</papertitle></a><br><strong>M. Nabi</strong>, S. Divvala, A. Farhadi<br>
                  <em>Technical Report</em>*, 2014 &nbsp; <br>
                  <a href="./files/UW2014_TR.pdf">PDF</a> /
                  <a href="./files/UW2014_slides.pdf">slide</a> /
                  <a href="#">code</a>
                </p><p></p>
                <p>We study discovering a set of discriminative patches in subcategories of an object category, then train them in a webly-supervised fashion.
</br></br>
<small>*This work was done while I was at UW.</small>
                </p><p></p>
                <p></p>
              </td>
            </tr>


<!--Next paper should be listed in bottom -->
	              <td width="25%"><img src="./img/Tposelet.jpg" alt="PontTuset" width="210" style="border-style: none">
	              </td><td width="75%" valign="top">
	                <p><a href="http://dl.acm.org/citation.cfm?id=2586288">
	<papertitle>Temporal Poselets for Collective Activity Detection and Recognition</papertitle></a><br><strong>M. Nabi</strong>, A. Del Bue, V. Murino<br>
                  <em>IEEE International Conference on Computer Vision Workshops</em>, 2013 &nbsp; <font color="red"><strong>(Oral)</strong></font><br>
                  <a href="./files/ICCVW2013_paper.pdf">PDF</a> /
                  <a href="./files/ICCVW2013_slides.pdf">slide</a> /
                  <a href="https://youtu.be/jkkVZqgm-UE">video</a> /
                  <a href="./files/ICCVW2013_code.tar.gz">code</a> /
                  <a href="./files/ICCVW2013.bib">bibtex</a>
                </p><p></p>
                <p>We present a mid-level motion representation based on actication patterns of Poselets detectors over time (Temporal Poselet), for detecting/recognizing the activity of a group of people in crowd environments.
		</p><p></p>
                <p></p>
              </td>
            </tr>


<!--Next paper should be listed in bottom -->
	              <td width="25%"><img src="./img/BagOfPoselet.jpg" alt="PontTuset" width="200" style="border-style: none">
	              </td><td width="75%" valign="top">
	                <p><a href="#">
	<papertitle>Human Action Recognition in Still Images using Bag of Latent Poselets</papertitle>*</a><br><strong>M. Nabi</strong>, M. Rahmati<br>
                  <em>9th European Conference on Visual Media Production (CVMP)</em>, 2012 &nbsp;<br>
                  <a href="./files/CVMP2012_abstract.pdf">PDF</a> /
                  <a href="./files/CVMP2012.bib">bibtex</a>
                </p><p></p>
                <p>We represent human body poses in a single images by extracting the Poselet activation vectors on it, and recognize human activities in still images using the proposed bag of latent Poselets.
</br></br>
<small>*This work is based on my MS thesis at AUT.</small>
		</p><p></p>
                <p></p>
              </td>
            </tr>


<!--SECTION -->

          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody><tr>
              <td>
                <heading>Miscellaneous</heading>
              </td>
            </tr>
          </tbody></table>
          <table width="100%" align="center" border="0" cellpadding="20">
            <tbody><tr>


<!--Next paper should be listed in bottom -->
	              <td width="25%"><img src="./img/PhDthesis.jpg" alt="PontTuset" width="160" style="border-style: none">
	              </td><td width="75%" valign="top">
	                <p><a href="http://arxiv.org/abs/1512.07314">
	<papertitle>Mid-level Representation for Visual Recognition</papertitle></a><br>
                  <strong>Moin Nabi</strong><br>
                  <em>Ph.D. Dissertation </em>, 2015 &nbsp; <br>
                  <a href="./files/PhD_thesis.pdf">PDF</a> /
                  <a href="./files/thesis_slide.pdf">slides</a> /
                  <a href="https://www.youtube.com/watch?v=6IY-0swKaiM">talk</a> /
                </p><p></p>
                <p>This thesis targets employing mid-level representations for different high-level visual
recognition tasks, both in image and video understanding.
                </p><p></p>
                <p></p>
              </td>
            </tr>

<!--Next paper should be listed in bottom -->
	              <td width="25%"><img src="./img/stock.jpg" alt="PontTuset" width="160" style="border-style: none">
	              </td><td width="75%" valign="top">
	                <p><a href="#">
	<papertitle>Stock trend prediction using Twin Gaussian Process regression</papertitle></a><br>M. Mojaddady, <strong>M. Nabi</strong>, S. Khadivi<br>
                  <em>Technical Report</em>, 2011 &nbsp;<br>
                  <a href="./files/stock.pdf">PDF</a> /
                  <a href="./files/stock.bib">bibtex</a>
                </p><p></p>
                <p>
		</p><p></p>
                <p></p>
              </td>
            </tr>

<!--Next paper should be listed in bottom -->
<!--
	              <td width="25%"><img src="./img/hafezedges.gif" alt="PontTuset" width="140" style="border-style: none">
	              </td><td width="75%" valign="top">
	                <p><a href="#">
	<papertitle>A Fuzzy Approach to Image Processing</papertitle></a><br><strong>M. Nabi</strong><br>
                  <em>Technical Report</em>, 2008 &nbsp;<br>
                  <a href="./files/fuzzy.pdf">PDF</a>
                </p><p></p>
                <p>
		</p><p></p>
                <p></p>
              </td>
            </tr>
-->

<!--Next paper should be listed in bottom -->
	              <td width="25%"><img src="./img/matlab3.jpg" alt="PontTuset" width="140" style="border-style: none">
	              </td><td width="75%" valign="top">
	                <p><a href="#">
	<papertitle>A Turorial on Digital Image Processing using MATLAB</papertitle></a><br><strong>M. Nabi</strong><br>
                  <em>National Digital Image Processing Workshop</em>, 2008 &nbsp;<br>
                  <a href="./files/tutorial.zip">PDF</a> /
		  <a href="./files/code.zip">code</a> /
		  <a href="./files/images.tar.gz">data</a>
                </p><p></p>
                <p>
		</p><p></p>
                <p></p>
              </td>
            </tr>




          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody><tr>
              <td>
                <br>
                <p align="right"><font size="3">
			Erd&ouml;s = 4 (via three paths)
                  </font>
                <br>
		<p align="right"><font size="2">
                  <a href="http://www.cs.berkeley.edu/~barron/">Thanks Jon!</a>
                  </font>
                </p>
              </td>
            </tr>
          </tbody></table>
          <script type="text/javascript">
            var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
                    document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
                    
          </script><script src="./img/ga.js" type="text/javascript"></script> <script type="text/javascript">
            try {
                    var pageTracker = _gat._getTracker("UA-7580334-1");
                    pageTracker._trackPageview();
                    } catch(err) {}
                    
          </script>
        </td>
      </tr>
    </tbody></table>
  

</body></html>
